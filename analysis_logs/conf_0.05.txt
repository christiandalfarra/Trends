Use GPU: 0 for training
Random initialization: initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
=> Model created: visual backbone RN50
=> Using native Torch AMP. Training in mixed precision.
evaluating: A
number of test samples: 7500
tpt_classification.py:149: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(init_scale=1000)
tpt_classification.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
tpt_classification.py:290: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
tpt_classification.py:48: RuntimeWarning: divide by zero encountered in log
  avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0]) # avg_logits = logits.mean(0) [1, 1000]
 *  Acc@1 17.787 Acc@5 42.707
=> Acc. on testset [A]: @1 17.786666870117188/ @5 42.706668853759766
======== Result Summary ========
params: nstep	lr	bs
params: 1	0.005	64
		 [set_id] 		 Top-1 acc. 		 Top-5 acc.
A	

17.79	

